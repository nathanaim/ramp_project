{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "from jours_feries_france import JoursFeries\n",
    "from datetime import datetime, date\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from astral import LocationInfo\n",
    "from astral.sun import daylight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(Path('data') / 'train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_daylight(x):\n",
    "\n",
    "    city=LocationInfo('Paris', timezone='Europe/Paris')\n",
    "    sun_info = daylight(city.observer, date=x.to_pydatetime().date(), tzinfo='Europe/Paris')\n",
    "    x = x.tz_localize('Europe/Paris', ambiguous=True, nonexistent='shift_forward')\n",
    "    return (x > sun_info[0]) & (x < sun_info[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _feature_engineering(data):\n",
    "    data = data.copy()\n",
    "    d = SchoolHolidayDates()\n",
    "    jf = JoursFeries()\n",
    "    data['hour'] = data.date.dt.hour\n",
    "    data['weekday'] = data.date.dt.weekday\n",
    "    data['dom'] = data.date.dt.day\n",
    "    data['week'] = data.date.dt.isocalendar().week\n",
    "    data['month'] = data.date.dt.month\n",
    "    data['year'] = data.date.dt.year\n",
    "    data['dom_counter'] = data.counter_installation_date.dt.day\n",
    "    data['month_counter'] = data.counter_installation_date.dt.month\n",
    "    data['year_counter'] = data.counter_installation_date.dt.year\n",
    "    data['date_datetime'] = data.date.map(lambda x: x.to_pydatetime().date())\n",
    "    data['is_ferie'] = data.date_datetime.map(lambda x: jf.is_bank_holiday(x, zone='Métropole'))\n",
    "    data['is_holiday'] = data.date_datetime.map(lambda x: d.is_holiday_for_zone(x, 'C'))\n",
    "    data.drop(columns=['counter_name', 'site_name', 'counter_technical_id', 'counter_installation_date', 'date_datetime'], inplace=True)\n",
    "    data['is_daylight'] = data.date.map(_is_daylight)\n",
    "    external_data = pd.read_csv(\"./submissions/external_data/external_data.csv\")\n",
    "    external_data.cl = external_data.cl.fillna(value=100)\n",
    "    external_data.cm = external_data.cm.fillna(value=100)\n",
    "    external_data.ch = external_data.ch.fillna(value=100)\n",
    "    external_data.ssfrai = external_data.ssfrai.fillna(value=0.0)\n",
    "    external_data.perssfrai = external_data.perssfrai.fillna(value=0.0)\n",
    "    external_data.dropna(axis=1, thresh=3000, inplace=True)\n",
    "    external_data.fillna(method='ffill', inplace=True)\n",
    "    external_data['datetime_date'] = pd.to_datetime(external_data.date)\n",
    "    external_data.drop(columns=['numer_sta', 'per', 'pres'], inplace=True)\n",
    "    ext_index = external_data.set_index('datetime_date')\n",
    "    ext_index.sort_index(inplace=True)\n",
    "    data_index = data.set_index('date')\n",
    "    data_index.sort_index(inplace=True)\n",
    "    merged_data = pd.merge_asof(data_index, ext_index, left_index=True, right_index=True)\n",
    "    merged_data['is_confinement_1'] = (merged_data.date > '2020-10-30') & (merged_data.date < '2020-12-15')\n",
    "    merged_data['is_confinement_2'] = (merged_data.date > '2021-04-03') & (merged_data.date < '2021-05-03')\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = _feature_engineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455163, 50)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455163, 193)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = ['counter_id', 'site_id', 'is_ferie', 'is_holiday', 'is_confinement_1', 'is_confinement_2', 'hour', 'weekday', 'month', 'year', 'is_daylight', 'cm', 'cl', 'ch']\n",
    "numerical_columns = ['latitude',\n",
    "                        'longitude',\n",
    "                        'dom',\n",
    "                        'week',\n",
    "                        'dom_counter',\n",
    "                        'month_counter',\n",
    "                        'year_counter',\n",
    "                        'pmer',\n",
    "                        'tend',\n",
    "                        'cod_tend',\n",
    "                        'dd',\n",
    "                        'ff',\n",
    "                        't',\n",
    "                        'td',\n",
    "                        'u',\n",
    "                        'vv',\n",
    "                        'ww',\n",
    "                        'w1',\n",
    "                        'w2',\n",
    "                        'n',\n",
    "                        'nbas',\n",
    "                        'tend24',\n",
    "                        'raf10',\n",
    "                        'rafper',\n",
    "                        'etat_sol',\n",
    "                        'ht_neige',\n",
    "                        'ssfrai',\n",
    "                        'perssfrai',\n",
    "                        'rr1',\n",
    "                        'rr3',\n",
    "                        'rr6',\n",
    "                        'rr12',\n",
    "                        'rr24']\n",
    "\n",
    "preprocessor = ColumnTransformer([('one_hot_encoder', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), categorical_columns),\n",
    "                                ('standard_scaler', StandardScaler(), numerical_columns)])\n",
    "\n",
    "test = preprocessor.fit_transform(merged_data)\n",
    "\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455163, 193)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_engineering = FunctionTransformer(_feature_engineering)\n",
    "\n",
    "pipe = make_pipeline(feature_engineering, preprocessor)\n",
    "\n",
    "test_2 = pipe.fit_transform(data)\n",
    "\n",
    "test_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['counter_id', 'site_id', 'bike_count', 'latitude', 'longitude',\n",
       "       'log_bike_count', 'hour', 'weekday', 'dom', 'week', 'month', 'year',\n",
       "       'dom_counter', 'month_counter', 'year_counter', 'is_ferie',\n",
       "       'is_holiday', 'is_daylight', 'date', 'pmer', 'tend', 'cod_tend', 'dd',\n",
       "       'ff', 't', 'td', 'u', 'vv', 'ww', 'w1', 'w2', 'n', 'nbas', 'cl', 'cm',\n",
       "       'ch', 'tend24', 'raf10', 'rafper', 'etat_sol', 'ht_neige', 'ssfrai',\n",
       "       'perssfrai', 'rr1', 'rr3', 'rr6', 'rr12', 'rr24', 'is_confinement_1',\n",
       "       'is_confinement_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "from jours_feries_france import JoursFeries\n",
    "from datetime import datetime, date\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from astral import LocationInfo\n",
    "from astral.sun import daylight\n",
    "\n",
    "def _is_daylight(x):\n",
    "\n",
    "    city=LocationInfo('Paris', timezone='Europe/Paris')\n",
    "    sun_info = daylight(city.observer, date=x.to_pydatetime().date(), tzinfo='Europe/Paris')\n",
    "    x = x.tz_localize('Europe/Paris', ambiguous=True, nonexistent='shift_forward')\n",
    "    return (x > sun_info[0]) & (x < sun_info[1])\n",
    "\n",
    "def _feature_engineering(data):\n",
    "    data = data.copy()\n",
    "    d = SchoolHolidayDates()\n",
    "    jf = JoursFeries()\n",
    "    data['hour'] = data.date.dt.hour\n",
    "    data['weekday'] = data.date.dt.weekday\n",
    "    data['dom'] = data.date.dt.day\n",
    "    data['week'] = data.date.dt.isocalendar().week\n",
    "    data['month'] = data.date.dt.month\n",
    "    data['year'] = data.date.dt.year\n",
    "    data['dom_counter'] = data.counter_installation_date.dt.day\n",
    "    data['month_counter'] = data.counter_installation_date.dt.month\n",
    "    data['year_counter'] = data.counter_installation_date.dt.year\n",
    "    data['date_datetime'] = data.date.map(lambda x: x.to_pydatetime().date())\n",
    "    data['is_ferie'] = data.date_datetime.map(lambda x: jf.is_bank_holiday(x, zone='Métropole'))\n",
    "    data['is_holiday'] = data.date_datetime.map(lambda x: d.is_holiday_for_zone(x, 'C'))\n",
    "    data.drop(columns=['counter_name', 'site_name', 'counter_technical_id', 'counter_installation_date', 'date_datetime'], inplace=True)\n",
    "    data['is_daylight'] = data.date.map(_is_daylight)\n",
    "    external_data = pd.read_csv(\"./submissions/external_data/external_data.csv\")\n",
    "    external_data.cl = external_data.cl.fillna(value=100)\n",
    "    external_data.cm = external_data.cm.fillna(value=100)\n",
    "    external_data.ch = external_data.ch.fillna(value=100)\n",
    "    external_data.ssfrai = external_data.ssfrai.fillna(value=0.0)\n",
    "    external_data.perssfrai = external_data.perssfrai.fillna(value=0.0)\n",
    "    external_data.dropna(axis=1, thresh=3000, inplace=True)\n",
    "    external_data.fillna(method='ffill', inplace=True)\n",
    "    external_data['datetime_date'] = pd.to_datetime(external_data.date)\n",
    "    external_data.drop(columns=['numer_sta', 'per', 'pres'], inplace=True)\n",
    "    ext_index = external_data.set_index('datetime_date')\n",
    "    ext_index.sort_index(inplace=True)\n",
    "    data_index = data.set_index('date')\n",
    "    data_index.sort_index(inplace=True)\n",
    "    merged_data = pd.merge_asof(data_index, ext_index, left_index=True, right_index=True)\n",
    "    merged_data['is_confinement_1'] = (merged_data.date > '2020-10-30') & (merged_data.date < '2020-12-15')\n",
    "    merged_data['is_confinement_2'] = (merged_data.date > '2021-04-03') & (merged_data.date < '2021-05-03')\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "def get_estimator():\n",
    "    feature_engineering=FunctionTransformer(_feature_engineering)\n",
    "\n",
    "    categorical_columns = ['counter_id', 'site_id', 'is_ferie', 'is_holiday', 'is_confinement_1', 'is_confinement_2', 'hour', 'weekday', 'month', 'year', 'is_daylight', 'cm', 'cl', 'ch']\n",
    "    numerical_columns = ['latitude',\n",
    "                        'longitude',\n",
    "                        'dom',\n",
    "                        'week',\n",
    "                        'dom_counter',\n",
    "                        'month_counter',\n",
    "                        'year_counter',\n",
    "                        'pmer',\n",
    "                        'tend',\n",
    "                        'cod_tend',\n",
    "                        'dd',\n",
    "                        'ff',\n",
    "                        't',\n",
    "                        'td',\n",
    "                        'u',\n",
    "                        'vv',\n",
    "                        'ww',\n",
    "                        'w1',\n",
    "                        'w2',\n",
    "                        'n',\n",
    "                        'nbas',\n",
    "                        'tend24',\n",
    "                        'raf10',\n",
    "                        'rafper',\n",
    "                        'etat_sol',\n",
    "                        'ht_neige',\n",
    "                        'ssfrai',\n",
    "                        'perssfrai',\n",
    "                        'rr1',\n",
    "                        'rr3',\n",
    "                        'rr6',\n",
    "                        'rr12',\n",
    "                        'rr24']\n",
    "\n",
    "    preprocessor = ColumnTransformer([('one_hot_encoder', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), categorical_columns),\n",
    "                                ('standard_scaler', StandardScaler(), numerical_columns)])\n",
    "\n",
    "    pipe = make_pipeline(feature_engineering, preprocessor)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(Path('data') / 'train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering=FunctionTransformer(_feature_engineering)\n",
    "\n",
    "categorical_columns = ['counter_id', 'site_id', 'is_ferie', 'is_holiday', 'is_confinement_1', 'is_confinement_2', 'hour', 'weekday', 'month', 'year', 'is_daylight', 'cm', 'cl', 'ch']\n",
    "numerical_columns = ['latitude',\n",
    "                    'longitude',\n",
    "                    'dom',\n",
    "                    'week',\n",
    "                    'dom_counter',\n",
    "                    'month_counter',\n",
    "                    'year_counter',\n",
    "                    'pmer',\n",
    "                    'tend',\n",
    "                    'cod_tend',\n",
    "                    'dd',\n",
    "                    'ff',\n",
    "                    't',\n",
    "                    'td',\n",
    "                    'u',\n",
    "                    'vv',\n",
    "                    'ww',\n",
    "                    'w1',\n",
    "                    'w2',\n",
    "                    'n',\n",
    "                    'nbas',\n",
    "                    'tend24',\n",
    "                    'raf10',\n",
    "                    'rafper',\n",
    "                    'etat_sol',\n",
    "                    'ht_neige',\n",
    "                    'ssfrai',\n",
    "                    'perssfrai',\n",
    "                    'rr1',\n",
    "                    'rr3',\n",
    "                    'rr6',\n",
    "                    'rr12',\n",
    "                    'rr24']\n",
    "\n",
    "preprocessor = ColumnTransformer([('one_hot_encoder', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), categorical_columns),\n",
    "                            ('standard_scaler', StandardScaler(), numerical_columns)])\n",
    "\n",
    "inputs = keras.Input(shape=(192,)) #TO BE CHANGED\n",
    "dense = layers.Dense(256, activation='relu')\n",
    "x = dense(inputs)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='bikes_deep')\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "pipe = make_pipeline(feature_engineering, preprocessor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"bikes_deep\" is incompatible with the layer: expected shape=(None, 192), found shape=(None, 193)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26648/3145172789.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\natha\\anaconda3\\envs\\bikes-ramp\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"bikes_deep\" is incompatible with the layer: expected shape=(None, 192), found shape=(None, 193)\n"
     ]
    }
   ],
   "source": [
    "test = pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455163,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test[:, 0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba7760a96a714ed40cd09cd10dc943384f9b3d9aad9713a41488818a15d139c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('bikes-ramp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
